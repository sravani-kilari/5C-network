{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPpfpkYTq0LZg/5vnBSPbaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravani-kilari/5C-network/blob/main/5C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "jsan_AvnlLAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the dataset path (update this with your folder path)\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Brain_MRI_Dataset\"\n"
      ],
      "metadata": {
        "id": "er49sLIwnwxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install FastAPI and Streamlit\n",
        "!pip install fastapi uvicorn nest-asyncio\n",
        "!pip install streamlit\n"
      ],
      "metadata": {
        "id": "ag9_rh2apDl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "ksb6nbArpZom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in the dataset directory\n",
        "import os\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Brain_MRI_Dataset\"  # Update this with your dataset path\n",
        "print(\"Listing files in dataset directory:\")\n",
        "print(os.listdir(DATASET_PATH))\n"
      ],
      "metadata": {
        "id": "zUf4NLQOpdO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the path to your dataset\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Brain_MRI_Dataset\"  # Update this path\n",
        "\n",
        "# List all files in the dataset directory\n",
        "files = os.listdir(DATASET_PATH)\n",
        "print(\"Files in dataset directory:\")\n",
        "for file in files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "1InsRF7lpq7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the path to your dataset\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Brain_MRI_Dataset\"  # Update this path\n",
        "\n",
        "# Check if the sample image exists\n",
        "sample_image = 'image1.png'  # Replace with your actual image file\n",
        "image_path = os.path.join(DATASET_PATH, sample_image)\n",
        "\n",
        "if os.path.isfile(image_path):\n",
        "    print(f\"Found image: {image_path}\")\n",
        "else:\n",
        "    print(f\"Image not found: {image_path}\")\n"
      ],
      "metadata": {
        "id": "yuzkjBhSqTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to load and display an image and its corresponding mask\n",
        "def load_and_display_sample(dataset_path, image_name):\n",
        "    img_path = os.path.join(dataset_path, image_name)\n",
        "    mask_path = img_path.replace('.png', '_mask.png').replace('.jpg', '_mask.jpg')  # Adjust according to your naming convention\n",
        "\n",
        "    # Load the image and mask\n",
        "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image and mask were loaded successfully\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image: {img_path}\")\n",
        "        return\n",
        "    if mask is None:\n",
        "        print(f\"Failed to load mask: {mask_path}\")\n",
        "        return\n",
        "\n",
        "    # Display the image and mask\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Image\")\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Mask\")\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function with a sample image name from your dataset\n",
        "load_and_display_sample(DATASET_PATH, sample_image)  # Replace with your actual image file\n"
      ],
      "metadata": {
        "id": "LeyAHXfPq4-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the path to your dataset\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Brain_MRI_Dataset\"  # Update this path\n",
        "\n",
        "# List all files in the dataset directory\n",
        "files = os.listdir(DATASET_PATH)\n",
        "print(\"Files in dataset directory:\")\n",
        "for file in files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "VvCIFmSurA1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Set the path to the zip file and the extraction directory\n",
        "zip_file_path = '/content/drive/MyDrive/Brain_MRI_Dataset/Data.zip'  # Update this path if necessary\n",
        "extraction_path = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data'  # Define an extraction path\n",
        "\n",
        "# Create extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(f\"Extracted files to: {extraction_path}\")\n"
      ],
      "metadata": {
        "id": "Ugh0UP6xrJWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the extracted dataset directory\n",
        "extracted_files = os.listdir(extraction_path)\n",
        "print(\"Files in extracted dataset directory:\")\n",
        "for file in extracted_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "JxXht2V3rSqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMBjkc9GsOFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w46ssupCsOsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PboDX-wusOva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JQ8FPjhsOzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to load and display an image\n",
        "def load_and_display_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.title(\"Image\")\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function with an actual filename from the extracted directory\n",
        "load_and_display_image(os.path.join(extraction_path, extracted_files[0]))  # Load the first file found\n",
        "\n"
      ],
      "metadata": {
        "id": "n6_tBsO6rYei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the 'Data' directory\n",
        "data_directory_path = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data'\n",
        "\n",
        "# List all files in the Data directory\n",
        "data_files = os.listdir(data_directory_path)\n",
        "print(\"Files in the Data directory:\")\n",
        "for file in data_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "ITS8E1pNsTKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to list all files recursively\n",
        "def list_files_recursively(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "\n",
        "# Call the function to list files in the 'Data' directory\n",
        "list_files_recursively(data_directory_path)\n"
      ],
      "metadata": {
        "id": "bEWnhPlTscNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and display an image\n",
        "def load_and_display_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.title(\"Image\")\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Replace 'your_image_file.png' with an actual image file found in the previous step\n",
        "sample_image_path = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data/your_image_file.png'  # Update this path\n",
        "load_and_display_image(sample_image_path)\n"
      ],
      "metadata": {
        "id": "MVMfbBqEseO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the path to one of the patient directories\n",
        "patient_directory = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data/TCGA_CS_6667_20011105'\n",
        "\n",
        "# List all files in the patient directory\n",
        "patient_files = os.listdir(patient_directory)\n",
        "print(\"Files in the patient directory:\")\n",
        "for file in patient_files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "WwS_FlUlslBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_data(patient_directory):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for file in os.listdir(patient_directory):\n",
        "        if file.endswith('.tif') and '_mask' not in file:\n",
        "            # Load image\n",
        "            image_path = os.path.join(patient_directory, file)\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            images.append(image)\n",
        "\n",
        "            # Load corresponding mask\n",
        "            mask_path = os.path.join(patient_directory, file.replace('.tif', '_mask.tif'))\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load data\n",
        "patient_directory = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data/TCGA_CS_6667_20011105'\n",
        "images, masks = load_data(patient_directory)\n",
        "\n",
        "print(f\"Loaded {len(images)} images and {len(masks)} masks.\")\n"
      ],
      "metadata": {
        "id": "RCHZsb8jslFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(images):\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    processed_images = []\n",
        "\n",
        "    for img in images:\n",
        "        img_clahe = clahe.apply(img)\n",
        "        processed_images.append(img_clahe)\n",
        "\n",
        "    return np.array(processed_images)\n",
        "\n",
        "# Preprocess images\n",
        "processed_images = preprocess_images(images)\n"
      ],
      "metadata": {
        "id": "fu67MTEvsyU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} images, {y_train.shape[0]} masks\")\n",
        "print(f\"Testing set: {X_test.shape[0]} images, {y_test.shape[0]} masks\")\n"
      ],
      "metadata": {
        "id": "XmpheItIs1W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ],
      "metadata": {
        "id": "OmnQa1uvs6nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List directories in your Google Drive\n",
        "base_path = '/content/drive/MyDrive/Brain_MRI_Dataset'  # Change this if needed\n",
        "print(os.listdir(base_path))\n"
      ],
      "metadata": {
        "id": "PJUVmgRgtGG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to your dataset folder\n",
        "base_path = '/content/drive/MyDrive/Brain_MRI_Dataset'  # Adjust if necessary\n",
        "\n",
        "# List the contents of the base directory\n",
        "print(\"Contents of the dataset directory:\")\n",
        "print(os.listdir(base_path))\n",
        "\n",
        "# If the \"Data\" folder is there, set the path correctly\n",
        "DATASET_PATH = os.path.join(base_path, 'Data')  # Ensure this matches exactly\n",
        "\n",
        "# Verify if the Data directory exists\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"Found dataset at: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"Dataset not found at: {DATASET_PATH}\")\n"
      ],
      "metadata": {
        "id": "TC6jxTQUtRQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the extracted_data directory\n",
        "extracted_data_path = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data'\n",
        "\n",
        "# List the contents of the extracted_data directory\n",
        "print(\"Contents of the extracted_data directory:\")\n",
        "print(os.listdir(extracted_data_path))\n"
      ],
      "metadata": {
        "id": "aPkvUcIytYMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = os.path.join(extracted_data_path, 'Data')  # Adjust accordingly based on the output\n"
      ],
      "metadata": {
        "id": "cdrfY2uxtbb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and masks from the correct path\n",
        "images, masks = load_images_and_masks(DATASET_PATH)\n",
        "\n",
        "# Verify if images and masks were loaded\n",
        "print(f'Loaded {len(images)} images and {len(masks)} masks.')\n"
      ],
      "metadata": {
        "id": "17aCnYP1tej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Brcfb7Uct85P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the extracted_data directory\n",
        "extracted_data_path = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data'\n",
        "\n",
        "# List the contents of the extracted_data directory\n",
        "print(\"Contents of the extracted_data directory:\")\n",
        "extracted_files = os.listdir(extracted_data_path)\n",
        "print(extracted_files)\n"
      ],
      "metadata": {
        "id": "FfJCz7sjtpQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_images(images, masks, img_size=(128, 128)):\n",
        "    processed_images = []\n",
        "    processed_masks = []\n",
        "\n",
        "    for img, mask in zip(images, masks):\n",
        "        # Resize images and masks\n",
        "        img_resized = cv2.resize(img, img_size)\n",
        "        mask_resized = cv2.resize(mask, img_size)\n",
        "\n",
        "        # Normalize images\n",
        "        img_normalized = img_resized / 255.0\n",
        "\n",
        "        processed_images.append(img_normalized)\n",
        "        processed_masks.append(mask_resized)\n",
        "\n",
        "    return np.array(processed_images), np.array(processed_masks)\n",
        "\n",
        "# Preprocess the loaded images and masks\n",
        "X, y = preprocess_images(images, masks)\n"
      ],
      "metadata": {
        "id": "tduI96xot-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def nested_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Implement the Nested U-Net architecture here\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "pL-SCvt5uBA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def nested_block(x, filters):\n",
        "    # Nested U-Net block with two convolutions and skip connections\n",
        "    conv1 = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.Activation('relu')(conv1)\n",
        "    conv1 = layers.Conv2D(filters, (3, 3), padding='same')(conv1)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.Activation('relu')(conv1)\n",
        "\n",
        "    # Adjust the number of channels in the skip connection using a 1x1 convolution\n",
        "    # if the number of channels in 'x' and 'conv1' are different.\n",
        "    if x.shape[-1] != conv1.shape[-1]:\n",
        "        x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
        "\n",
        "    # Skip connection\n",
        "    x_skip = layers.add([x, conv1])\n",
        "\n",
        "    return x_skip\n",
        "\n",
        "def nested_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Downsampling path\n",
        "    block1 = nested_block(inputs, 32)\n",
        "    pool1 = layers.MaxPooling2D((2, 2))(block1)\n",
        "\n",
        "    block2 = nested_block(pool1, 64)\n",
        "    pool2 = layers.MaxPooling2D((2, 2))(block2)\n",
        "\n",
        "    block3 = nested_block(pool2, 128)\n",
        "    pool3 = layers.MaxPooling2D((2, 2))(block3)\n",
        "\n",
        "    block4 = nested_block(pool3, 256)\n",
        "    pool4 = layers.MaxPooling2D((2, 2))(block4)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = nested_block(pool4, 512)\n",
        "\n",
        "    # Upsampling path\n",
        "    up4 = layers.UpSampling2D((2, 2))(bottleneck)\n",
        "    concat4 = layers.concatenate([up4, block4])\n",
        "    block5 = nested_block(concat4, 256)\n",
        "\n",
        "    up3 = layers.UpSampling2D((2, 2))(block5)\n",
        "    concat3 = layers.concatenate([up3, block3])\n",
        "    block6 = nested_block(concat3, 128)\n",
        "\n",
        "    up2 = layers.UpSampling2D((2, 2))(block6)\n",
        "    concat2 = layers.concatenate([up2, block2])\n",
        "    block7 = nested_block(concat2, 64)\n",
        "\n",
        "    up1 = layers.UpSampling2D((2, 2))(block7)\n",
        "    concat1 = layers.concatenate([up1, block1])\n",
        "    block8 = nested_block(concat1, 32)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(block8)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "model_nested = nested_unet((128, 128, 1))\n",
        "\n",
        "# Compile the model\n",
        "model_nested.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Now you can train the model as before\n",
        "# history_nested = model_nested.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n"
      ],
      "metadata": {
        "id": "YERmJvTHuEDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming images and masks are loaded into `images` and `masks`\n",
        "images = images.astype('float32') / 255.0  # Normalize images\n",
        "masks = masks.astype('float32')  # Ensure masks are in float32 for loss calculation\n"
      ],
      "metadata": {
        "id": "lounBQK6usCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to load images and masks\n",
        "def load_images_and_masks(data_dir):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Iterate through the patient directories\n",
        "    for patient_dir in os.listdir(data_dir):\n",
        "        patient_path = os.path.join(data_dir, patient_dir)\n",
        "\n",
        "        if os.path.isdir(patient_path):  # Ensure it's a directory\n",
        "            image_files = [f for f in os.listdir(patient_path) if f.endswith('.tif') and '_mask' not in f]\n",
        "            mask_files = [f for f in os.listdir(patient_path) if f.endswith('.tif') and '_mask' in f]\n",
        "\n",
        "            for img_file in image_files:\n",
        "                img_path = os.path.join(patient_path, img_file)\n",
        "                img = img_to_array(load_img(img_path, color_mode='grayscale'))  # Load image\n",
        "\n",
        "                # Find corresponding mask file\n",
        "                mask_file = img_file.replace('.tif', '_mask.tif')\n",
        "                if mask_file in mask_files:\n",
        "                    mask_path = os.path.join(patient_path, mask_file)\n",
        "                    mask = img_to_array(load_img(mask_path, color_mode='grayscale'))  # Load mask\n",
        "\n",
        "                    images.append(img)\n",
        "                    masks.append(mask)\n",
        "                else:\n",
        "                    print(f\"Warning: No mask found for {img_file}\")\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load the dataset\n",
        "DATASET_PATH = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data'  # Adjust the path as necessary\n",
        "images, masks = load_images_and_masks(DATASET_PATH)\n",
        "\n",
        "# Print the number of loaded images and masks\n",
        "print(f'Loaded {len(images)} images and {len(masks)} masks.')\n",
        "\n",
        "# Check if the number of images and masks is the same\n",
        "num_images = len(images)\n",
        "num_masks = len(masks)\n",
        "\n",
        "if num_images != num_masks:\n",
        "    # Print shapes of loaded arrays\n",
        "    print(f\"Error: Number of images ({num_images}) and masks ({num_masks}) is not the same.\")\n",
        "    print(\"Shape of images:\", images.shape)\n",
        "    print(\"Shape of masks:\", masks.shape)\n",
        "else:\n",
        "    # Split the dataset if counts are consistent\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "    print(\"Dataset split successfully.\")\n"
      ],
      "metadata": {
        "id": "yMohor62uvvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to load images and masks\n",
        "def load_images_and_masks(data_dir):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Iterate through the patient directories\n",
        "    for patient_dir in os.listdir(data_dir):\n",
        "        patient_path = os.path.join(data_dir, patient_dir)\n",
        "\n",
        "        # Check if it is a directory and not a hidden directory\n",
        "        if os.path.isdir(patient_path) and not patient_dir.startswith('.'):\n",
        "            print(f\"Loading data from: {patient_path}\")\n",
        "            # Load images and masks from the patient directory\n",
        "            for img_file in os.listdir(patient_path):\n",
        "                # Load images\n",
        "                if img_file.endswith('.tif') and 'mask' not in img_file:\n",
        "                    img_path = os.path.join(patient_path, img_file)\n",
        "                    img = load_img(img_path, color_mode='grayscale')\n",
        "                    img = img_to_array(img)\n",
        "                    images.append(img)\n",
        "\n",
        "                # Load masks\n",
        "                if img_file.endswith('_mask.tif'):\n",
        "                    mask_path = os.path.join(patient_path, img_file)\n",
        "                    mask = load_img(mask_path, color_mode='grayscale')\n",
        "                    mask = img_to_array(mask)\n",
        "                    masks.append(mask)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    images = np.array(images)\n",
        "    masks = np.array(masks)\n",
        "\n",
        "    # Debugging output to check shapes\n",
        "    print(f\"Loaded {len(images)} images and {len(masks)} masks.\")\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "# Load data\n",
        "DATASET_PATH = '/content/drive/MyDrive/Brain_MRI_Dataset/extracted_data/Data'  # Adjust to your path\n",
        "images, masks = load_images_and_masks(DATASET_PATH)\n",
        "\n",
        "# Check if the number of images and masks is the same\n",
        "num_images = len(images)\n",
        "num_masks = len(masks)\n",
        "\n",
        "if num_images != num_masks:\n",
        "    print(f\"Error: Number of images ({num_images}) and masks ({num_masks}) is not the same.\")\n",
        "else:\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "    print(\"Dataset split successfully.\")\n",
        "\n",
        "    # Preprocessing\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Assuming binary segmentation, convert masks to binary format\n",
        "    y_train = (y_train > 0).astype(np.uint8)  # Assuming masks are non-binary and need thresholding\n",
        "    y_test = (y_test > 0).astype(np.uint8)\n",
        "\n",
        "    # Reshape if necessary\n",
        "    X_train = np.resize(X_train, (X_train.shape[0], 128, 128, 1))  # Add channel dimension if required\n",
        "    X_test = np.resize(X_test, (X_test.shape[0], 128, 128, 1))\n",
        "    y_train = np.resize(y_train, (y_train.shape[0], 128, 128, 1))\n",
        "    y_test = np.resize(y_test, (y_test.shape[0], 128, 128, 1))\n",
        "\n",
        "    print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
        "    print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "# Continue with model definition, compilation, and training...\n"
      ],
      "metadata": {
        "id": "WEg5TH6ovZHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `model_nested` is your initialized model\n",
        "model_nested.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model_nested.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "id": "uLRJyFiyuvsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to float32 and normalize the images\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert masks to binary format if necessary\n",
        "# This assumes that your masks contain pixel values of 0 or 255; adjust if they contain different values\n",
        "y_train = (y_train > 0).astype(np.uint8)\n",
        "y_test = (y_test > 0).astype(np.uint8)\n",
        "\n",
        "# Resize images and masks if needed (example: resizing to 128x128)\n",
        "X_train = np.resize(X_train, (X_train.shape[0], 128, 128, 1))  # Add channel dimension if required\n",
        "X_test = np.resize(X_test, (X_test.shape[0], 128, 128, 1))\n",
        "y_train = np.resize(y_train, (y_train.shape[0], 128, 128, 1))\n",
        "y_test = np.resize(y_test, (y_test.shape[0], 128, 128, 1))\n"
      ],
      "metadata": {
        "id": "McGQ_QUQvkNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def nested_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Define your U-Net architecture here, similar to the one we've discussed earlier.\n",
        "    # For example, you could include nested blocks as previously defined.\n",
        "\n",
        "    # Final output layer\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)  # Use 'softmax' if multi-class segmentation\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "qSbhl_4Owg5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def nested_block(x, filters):\n",
        "    # First convolution\n",
        "    conv1 = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.Activation('relu')(conv1)\n",
        "    conv1 = layers.Conv2D(filters, (3, 3), padding='same')(conv1)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.Activation('relu')(conv1)\n",
        "\n",
        "    # Adjust the number of channels in the skip connection using a 1x1 convolution\n",
        "    if x.shape[-1] != conv1.shape[-1]:\n",
        "        x = layers.Conv2D(filters, (1, 1), padding='same')(x)\n",
        "\n",
        "    # Skip connection\n",
        "    x_skip = layers.add([x, conv1])\n",
        "\n",
        "    return x_skip\n"
      ],
      "metadata": {
        "id": "mPgvE0fwwj07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_unet(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder path\n",
        "    x1 = nested_block(inputs, filters=32)\n",
        "    x2 = layers.MaxPooling2D((2, 2))(x1)\n",
        "\n",
        "    x2 = nested_block(x2, filters=64)\n",
        "    x3 = layers.MaxPooling2D((2, 2))(x2)\n",
        "\n",
        "    x3 = nested_block(x3, filters=128)\n",
        "    x4 = layers.MaxPooling2D((2, 2))(x3)\n",
        "\n",
        "    x4 = nested_block(x4, filters=256)\n",
        "    x5 = layers.MaxPooling2D((2, 2))(x4)\n",
        "\n",
        "    x5 = nested_block(x5, filters=512)\n",
        "\n",
        "    # Decoder path\n",
        "    x = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x5)\n",
        "    x = layers.concatenate([x, x4])\n",
        "    x = nested_block(x, filters=256)\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.concatenate([x, x3])\n",
        "    x = nested_block(x, filters=128)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.concatenate([x, x2])\n",
        "    x = nested_block(x, filters=64)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.concatenate([x, x1])\n",
        "    x = nested_block(x, filters=32)\n",
        "\n",
        "    # Final output layer\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)  # Use 'softmax' if multi-class segmentation\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SC01PviRwjwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = nested_unet((128, 128, 1))  # Assuming grayscale images\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                    batch_size=16, epochs=50)  # Adjust batch size and epochs as necessary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiBkoaVQwjkV",
        "outputId": "e7993fcf-4dd8-4f1a-bc9e-0ee515e2b4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m 89/197\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m21:14\u001b[0m 12s/step - accuracy: 0.9167 - loss: 0.1889"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "BFS9qn-mxojY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predictions to binary (thresholding)\n",
        "y_pred_binary = (y_pred > 0.5).astype(np.uint8)  # Assuming a binary segmentation task\n"
      ],
      "metadata": {
        "id": "20j6E37Oxo4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot images, true masks, and predicted masks\n",
        "def plot_results(X, y_true, y_pred, num_images=5):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(num_images, 3, 3 * i + 1)\n",
        "        plt.imshow(X[i].reshape(128, 128), cmap='gray')\n",
        "        plt.title('Input Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_images, 3, 3 * i + 2)\n",
        "        plt.imshow(y_true[i].reshape(128, 128), cmap='gray')\n",
        "        plt.title('True Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(num_images, 3, 3 * i + 3)\n",
        "        plt.imshow(y_pred[i].reshape(128, 128), cmap='gray')\n",
        "        plt.title('Predicted Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the first 5 images, their true masks, and the predicted masks\n",
        "plot_results(X_test, y_test, y_pred_binary, num_images=5)\n"
      ],
      "metadata": {
        "id": "aBuhQGbAx0fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('nested_unet_brain_mri.h5')\n"
      ],
      "metadata": {
        "id": "hwkWMV4-x4ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "# Function to calculate Dice Coefficient\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    return 2. * intersection / (np.sum(y_true) + np.sum(y_pred) + 1e-6)\n",
        "\n",
        "# Example metrics calculation\n",
        "dice_scores = [dice_coefficient(y_test[i].flatten(), y_pred_binary[i].flatten()) for i in range(len(y_test))]\n",
        "mean_dice_score = np.mean(dice_scores)\n",
        "\n",
        "print(f'Mean Dice Score: {mean_dice_score}')\n"
      ],
      "metadata": {
        "id": "3_4ybG0ByD_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "y_test_flat = y_test.flatten()\n",
        "y_pred_flat = y_pred_binary.flatten()\n",
        "\n",
        "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Background', 'Foreground'],\n",
        "            yticklabels=['Background', 'Foreground'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OMPPh7ycyGev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n"
      ],
      "metadata": {
        "id": "kjy3G7-WyILh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "riEi8Tu_yIHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of adjusting batch size and number of epochs\n",
        "batch_size = 16\n",
        "num_epochs = 50\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    callbacks=[early_stopper, lr_reducer])\n"
      ],
      "metadata": {
        "id": "ff778gNhyID9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit generator can be used with model.fit for more robust training\n",
        "model.fit(data_gen.flow(X_train, y_train, batch_size=batch_size),\n",
        "          validation_data=(X_test, y_test),\n",
        "          epochs=num_epochs,\n",
        "          callbacks=[early_stopper, lr_reducer])\n"
      ],
      "metadata": {
        "id": "ss85mMSoyIAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nrMUgl4byHfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(X, y_true, y_pred, num_images=5):\n",
        "    for i in range(num_images):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(X[i].squeeze(), cmap='gray')\n",
        "        plt.title('Input Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(y_true[i].squeeze(), cmap='gray')\n",
        "        plt.title('True Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(y_pred[i].squeeze(), cmap='gray')\n",
        "        plt.title('Predicted Mask')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Call the function\n",
        "display_results(X_test, y_test, y_pred_binary)\n"
      ],
      "metadata": {
        "id": "IyUqbzWVy4cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = load_model('nested_unet_brain_segmentation.h5')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    file = request.files['image']\n",
        "    img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (128, 128))  # Resize to match model input\n",
        "    img = img.reshape(1, 128, 128, 1) / 255.0  # Normalize\n",
        "\n",
        "    pred = model.predict(img)\n",
        "    pred_mask = (pred.squeeze() > 0.5).astype(np.uint8)  # Binarize prediction\n",
        "\n",
        "    return jsonify({'mask': pred_mask.tolist()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "BagZLklFzDtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n"
      ],
      "metadata": {
        "id": "yYPrYjjJzcnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "UAHToDJezezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Display results\n",
        "for i in range(5):  # Displaying 5 examples\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(X_test[i].squeeze(), cmap='gray')\n",
        "    plt.title('Input Image')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(y_test[i].squeeze(), cmap='gray')\n",
        "    plt.title('True Mask')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Dtzpp60XzhN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('segmentation_model.h5')\n"
      ],
      "metadata": {
        "id": "uSTTZo5tzjdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}